{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines for the Twitter Sentiment Analysis CIL Project\n",
    "\n",
    "They will mostly employ simple linear classifiers on naive operations on the word embeddings, such as averaging.\n",
    "\n",
    " * Attempt 1. uses word embedding vector averaging.\n",
    " * Attempt 2. uses embedding vector concatenation and is VERY memory-hungry.\n",
    " * The `del`s in the code seek to alleviate some of the memory pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will work on the preprocessed data, so that we have a common ground with\n",
    "# the deep learning competitors.\n",
    "pp = os.path.join('..', 'data', 'preprocessing')\n",
    "\n",
    "# How many Tweets to sample.\n",
    "# Averaging should work with all of them, but concatenation chokes\n",
    "# on even a tenth in its current implementation.\n",
    "LIMIT = 250000\n",
    "\n",
    "trainX = np.load(os.path.join(pp, 'full-trainX.npy'))\n",
    "trainY = np.load(os.path.join(pp, 'full-trainY.npy'))\n",
    "embeddings = np.load(os.path.join(pp, 'full-embeddings.npy'))\n",
    "\n",
    "trainY = np.argmax(trainY, axis=1)\n",
    "trainX, trainY = shuffle(trainX, trainY)\n",
    "trainX = trainX[:LIMIT]\n",
    "trainY = trainY[:LIMIT]\n",
    "\n",
    "with open(os.path.join(pp, 'full-vocab.pkl'), 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_avg(tweet):\n",
    "    \"\"\"Averages a tweet's representation vectors.\n",
    "    \n",
    "    Ignores the padding.\n",
    "    \"\"\"\n",
    "    return np.mean([embeddings[wid] for wid in tweet if wid != 0], axis=0)\n",
    "\n",
    "def mk_concat(tweet, lim=25):\n",
    "    \"\"\"Concatenates the word embeddings in a tweet.\n",
    "    \n",
    "    Does not ignore the padding.\n",
    "    \"\"\"\n",
    "    return np.hstack([embeddings[wid] for wid in tweet[:lim]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import *\n",
    "\n",
    "grid = {\n",
    "    'alpha': [0.00001, 0.00005, 0.0001, 0.0005, 0.001],\n",
    "}\n",
    "\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=lambda x: x[1], reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"{2}: Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores),\n",
    "              i + 1))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_avg(trainX, trainY):\n",
    "    \"\"\"Averages each tweet's word vectors into one, and trains a linear classifier.\"\"\"\n",
    "\n",
    "    print(\"Evaluating input of size {0}.\".format(trainX.shape))\n",
    "    print(\"Doing embedding averaging.\")\n",
    "    avgd_tweets = [mk_avg(tweet) for tweet in trainX]\n",
    "    avgd_tweets = [(t, label) for (t, label) in zip(avgd_tweets, trainY) if t.shape == (300,)]\n",
    "    trainX_trimmed = np.array([t for (t, label) in avgd_tweets])\n",
    "    trainY_trimmed = np.array([label for (t, label) in avgd_tweets])\n",
    "    print(trainX_trimmed.shape)\n",
    "    print(trainY_trimmed.shape)\n",
    "    del avgd_tweets\n",
    "    trainX_trimmed, trainY_trimmed = shuffle(trainX_trimmed, trainY_trimmed)\n",
    "    \n",
    "    gs = GridSearchCV(SGDClassifier(), grid, cv=3, verbose=True)\n",
    "    print(\"Starting grid search...\")\n",
    "    res = gs.fit(trainX_trimmed, trainY_trimmed)\n",
    "    report(res.grid_scores_, n_top=25)\n",
    "    \n",
    "    predY = res.predict(trainX_trimmed)\n",
    "    acc = accuracy_score(trainY_trimmed, predY)\n",
    "    f1 = accuracy_score(trainY_trimmed, predY)\n",
    "    \n",
    "    print(\"Train accuracy: {0}\\nTrain F1 score: {1}\".format(acc, f1))\n",
    "    \n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_concat(trainX, trainY):\n",
    "    \"\"\"Concatenates each tweet's word vectors into one, and trains a linear classifier.\n",
    "    \n",
    "    Note: can get VERY memory-hungry.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Evaluating input of size {0}.\".format(trainX.shape))\n",
    "    print(\"Doing embedding concatenation.\")\n",
    "    \n",
    "    embedding_size = 300\n",
    "    max_tweet_len = 25\n",
    "    \n",
    "    concat_tweets = [mk_concat(tweet, max_tweet_len) for tweet in trainX]\n",
    "    concat_tweets = [(t, label) for (t, label) in zip(concat_tweets, trainY) if t.shape == (embedding_size * max_tweet_len,)]\n",
    "    trainX_trimmed = np.array([t for (t, label) in concat_tweets])\n",
    "    trainY_trimmed = np.array([label for (t, label) in concat_tweets])\n",
    "    print(trainX_trimmed.shape)\n",
    "    print(trainY_trimmed.shape)\n",
    "    del concat_tweets\n",
    "    trainX_trimmed, trainY_trimmed = shuffle(trainX_trimmed, trainY_trimmed)\n",
    "    \n",
    "    gs = GridSearchCV(SGDClassifier(), grid, cv=3, verbose=True)\n",
    "    print(\"Starting grid search...\")\n",
    "    res = gs.fit(trainX_trimmed, trainY_trimmed)\n",
    "    report(res.grid_scores_, n_top=25)\n",
    "    \n",
    "    predY = res.predict(trainX_trimmed)\n",
    "    acc = accuracy_score(trainY_trimmed, predY)\n",
    "    f1 = accuracy_score(trainY_trimmed, predY)\n",
    "    \n",
    "    print(\"Train accuracy: {0}\\nTrain F1 score: {1}\".format(acc, f1))\n",
    "    \n",
    "    return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating input of size (2499999, 40).\n",
      "Doing embedding averaging.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrei/anaconda3/envs/cil/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/Users/andrei/anaconda3/envs/cil/lib/python3.5/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2499981, 300)\n",
      "(2499981,)\n",
      "Starting grid search...\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Mean validation score: 0.774 (std: 0.000)\n",
      "Parameters: {'alpha': 5e-05}\n",
      "\n",
      "2: Mean validation score: 0.772 (std: 0.003)\n",
      "Parameters: {'alpha': 1e-05}\n",
      "\n",
      "3: Mean validation score: 0.772 (std: 0.000)\n",
      "Parameters: {'alpha': 0.0001}\n",
      "\n",
      "4: Mean validation score: 0.764 (std: 0.000)\n",
      "Parameters: {'alpha': 0.0005}\n",
      "\n",
      "5: Mean validation score: 0.757 (std: 0.000)\n",
      "Parameters: {'alpha': 0.001}\n",
      "\n",
      "Train accuracy: 0.7739754822136649\n",
      "Train F1 score: 0.7739754822136649\n"
     ]
    }
   ],
   "source": [
    "avg_res = eval_avg(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating input of size (250000, 40).\n",
      "Doing embedding concatenation.\n",
      "(250000, 7500)\n",
      "(250000,)\n",
      "Starting grid search...\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 87.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Mean validation score: 0.774 (std: 0.001)\n",
      "Parameters: {'alpha': 0.001}\n",
      "\n",
      "2: Mean validation score: 0.773 (std: 0.001)\n",
      "Parameters: {'alpha': 0.0005}\n",
      "\n",
      "3: Mean validation score: 0.751 (std: 0.006)\n",
      "Parameters: {'alpha': 0.0001}\n",
      "\n",
      "4: Mean validation score: 0.734 (std: 0.012)\n",
      "Parameters: {'alpha': 5e-05}\n",
      "\n",
      "5: Mean validation score: 0.723 (std: 0.013)\n",
      "Parameters: {'alpha': 1e-05}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WARNING: Can get VERY memory-hungry!\n",
    "\n",
    "concat_res = eval_concat(trainX, trainY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
