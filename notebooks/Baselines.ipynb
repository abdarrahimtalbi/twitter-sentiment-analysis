{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines for the Twitter Sentiment Analysis CIL Project\n",
    "\n",
    "They will mostly employ simple linear classifiers on naive operations on the word embeddings, such as averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will work on the preprocessed data, so that we have a common ground with\n",
    "# the deep learning competitors.\n",
    "pp = os.path.join('..', 'data', 'preprocessing')\n",
    "\n",
    "LIMIT = -1\n",
    "\n",
    "trainX = np.load(os.path.join(pp, 'full-trainX.npy'))\n",
    "trainY = np.load(os.path.join(pp, 'full-trainY.npy'))\n",
    "embeddings = np.load(os.path.join(pp, 'full-embeddings.npy'))\n",
    "\n",
    "trainY = np.argmax(trainY, axis=1)\n",
    "trainX, trainY = shuffle(trainX, trainY)\n",
    "trainX = trainX[:LIMIT]\n",
    "trainY = trainY[:LIMIT]\n",
    "\n",
    "with open(os.path.join(pp, 'full-vocab.pkl'), 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mk_avg(tweet):\n",
    "    \"\"\"Averages a tweet's representation vectors.\n",
    "    \n",
    "    Ignores the padding.\n",
    "    \"\"\"\n",
    "    return np.mean([embeddings[wid] for wid in tweet if wid != 0], axis=0)\n",
    "\n",
    "def mk_concat(tweet, lim=15):\n",
    "    \"\"\"Concatenates the word embeddings in a tweet.\n",
    "    \n",
    "    Does not ignore the padding.\n",
    "    \"\"\"\n",
    "    return np.hstack([embeddings[wid] for wid in tweet[:lim]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts = mk_concat(trainX[128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import *\n",
    "\n",
    "grid = {\n",
    "    'alpha': [0.00001, 0.00005, 0.0001, 0.0005, 0.001],\n",
    "}\n",
    "\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=lambda x: x[1], reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"{2}: Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores),\n",
    "              i + 1))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "def eval_avg(trainX, trainY):\n",
    "    print(\"Evaluating input of size {0}.\".format(trainX.shape))\n",
    "    print(\"Doing embedding averaging.\")\n",
    "    avgd_tweets = [mk_avg(tweet) for tweet in trainX]\n",
    "    avgd_tweets = [(t, label) for (t, label) in zip(avgd_tweets, trainY) if t.shape == (300,)]\n",
    "    trainX_trimmed = np.array([t for (t, label) in avgd_tweets])\n",
    "    trainY_trimmed = np.array([label for (t, label) in avgd_tweets])\n",
    "    print(trainX_trimmed.shape)\n",
    "    print(trainY_trimmed.shape)\n",
    "    del avgd_tweets\n",
    "    trainX_trimmed, trainY_trimmed = shuffle(trainX_trimmed, trainY_trimmed)\n",
    "    \n",
    "    gs = GridSearchCV(SGDClassifier(), grid, cv=3, verbose=True)\n",
    "    print(\"Starting grid search...\")\n",
    "    res = gs.fit(trainX_trimmed, trainY_trimmed)\n",
    "    report(res.grid_scores_, n_top=25)\n",
    "    \n",
    "    predY = res.predict(trainX_trimmed)\n",
    "    acc = accuracy_score(trainY_trimmed, predY)\n",
    "    f1 = accuracy_score(trainY_trimmed, predY)\n",
    "    \n",
    "    print(\"Train accuracy: {0}\\nTrain F1 score: {1}\".format(acc, f1))\n",
    "    \n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_concat(trainX, trainY):\n",
    "    print(\"Evaluating input of size {0}.\".format(trainX.shape))\n",
    "    print(\"Doing embedding concatenation.\")\n",
    "    concat_tweets = [mk_concat(tweet) for tweet in trainX]\n",
    "    concat_tweets = [(t, label) for (t, label) in zip(concat_tweets, trainY) if t.shape == (300 * 15,)]\n",
    "    trainX_trimmed = np.array([t for (t, label) in concat_tweets])\n",
    "    trainY_trimmed = np.array([label for (t, label) in concat_tweets])\n",
    "    print(trainX_trimmed.shape)\n",
    "    print(trainY_trimmed.shape)\n",
    "    del concat_tweets\n",
    "    trainX_trimmed, trainY_trimmed = shuffle(trainX_trimmed, trainY_trimmed)\n",
    "    \n",
    "    gs = GridSearchCV(SGDClassifier(), grid, cv=3, n_jobs=1, verbose=True)\n",
    "    print(\"Starting grid search...\")\n",
    "    res = gs.fit(trainX_trimmed, trainY_trimmed)\n",
    "    report(res.grid_scores_, n_top=25)\n",
    "    return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating input of size (2499999, 40).\n",
      "Doing embedding averaging.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrei/anaconda3/envs/cil/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/Users/andrei/anaconda3/envs/cil/lib/python3.5/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "avg_res = eval_avg(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WARNING: Can get VERY memory-hungry!\n",
    "\n",
    "concat_res = eval_concat(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
