# Default LR was 0.0001.
# Set LR = 0.000075 but doesn't seem to affect the training.
# Set LR = 0.001 and the results were shit.

# Set embedding size == 350. Results worse than pre-trained w2v.
# Set embedding size == 275. Results even worse.
# Set embedding size == 225. Yep, even worse.

* Embedding size == 350
    * 3,4,5,7 filter sizes and 256 of each. (0.85580)

## Experiments to do
 * Embedding size 400 (will be slower to train).
 * Submit local-trained 225-d results to kaggle. Should be worse than 275-d.
 * Keep embeddings at 300 or 350 and use larger filter size.
 * Moar preprocessing.
 * Another convolution layer?
